{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a222ea",
   "metadata": {},
   "source": [
    "## Loading 20NG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732e7620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "732e7620",
    "outputId": "b738fd24-612a-4a0e-ea3c-262d19f06160",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       comp.os_ms_windows_misc  religion.rmisc  rec.sport.baseball  sci.space  \\\n",
      "0                            0               0                   0          0   \n",
      "1                            0               0                   0          0   \n",
      "2                            0               0                   0          0   \n",
      "3                            0               0                   0          0   \n",
      "4                            0               0                 1.0          0   \n",
      "...                        ...             ...                 ...        ...   \n",
      "19296                        0               0                   0          0   \n",
      "19297                        0               0                   0          0   \n",
      "19298                        0               0                   0          0   \n",
      "19299                        0               0                   0          0   \n",
      "19300                        0               0                   0          0   \n",
      "\n",
      "       comp.sys.mac_hardware  sci.med  politics.pmisc  rec.autos  \\\n",
      "0                          0        0               0          0   \n",
      "1                          0        0             1.0          0   \n",
      "2                          0        0               0          0   \n",
      "3                          0        0               0          0   \n",
      "4                          0        0               0          0   \n",
      "...                      ...      ...             ...        ...   \n",
      "19296                      0        0               0          0   \n",
      "19297                      0        0               0          0   \n",
      "19298                      0        0               0          0   \n",
      "19299                      0        0               0          0   \n",
      "19300                      0        0               0          0   \n",
      "\n",
      "       misc_forsale  politics.mideast  ...  written  wrong  wrote  x11  xx  \\\n",
      "0                 0                 0  ...        0      0      0    0   0   \n",
      "1                 0                 0  ...        0      0      0    0   0   \n",
      "2                 0                 0  ...        0      0      0    0   0   \n",
      "3               1.0                 0  ...        0      0      0    0   0   \n",
      "4                 0                 0  ...        0      0      0    0   0   \n",
      "...             ...               ...  ...      ...    ...    ...  ...  ..   \n",
      "19296             0                 0  ...        0      0      0    0   0   \n",
      "19297             0                 0  ...        0      0      0    0   0   \n",
      "19298             0                 0  ...        0      0      0    0   0   \n",
      "19299             0                 0  ...        0      0      0    0   0   \n",
      "19300             0                 0  ...        0      0      0    0   0   \n",
      "\n",
      "       yeah  year  years  york  young  \n",
      "0         0     0      0     0      0  \n",
      "1         0     0      0     0      0  \n",
      "2         0     0      0     0      0  \n",
      "3         0     0      0     0      0  \n",
      "4         0   1.0      0     0    1.0  \n",
      "...     ...   ...    ...   ...    ...  \n",
      "19296     0     0      0     0      0  \n",
      "19297     0     0      0     0      0  \n",
      "19298     0     0      0     0      0  \n",
      "19299     0     0      0     0      0  \n",
      "19300     0     0      0     0      0  \n",
      "\n",
      "[19301 rows x 1026 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "import re\n",
    "\n",
    "# Path to the ARFF file\n",
    "arff_file_path = r'20NG-F.arff'\n",
    "\n",
    "# Read the ARFF file\n",
    "with open(arff_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Initialize variables to store the attributes and data\n",
    "attributes = []\n",
    "data_lines = []\n",
    "\n",
    "# Flag to indicate when data section starts\n",
    "data_section = False\n",
    "\n",
    "# Process each line to extract attributes and data\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith('@attribute'):\n",
    "        attr_name = line.split()[1]\n",
    "        attributes.append(attr_name)\n",
    "    elif line.startswith('@data'):\n",
    "        data_section = True\n",
    "    elif data_section:\n",
    "        data_lines.append(line)\n",
    "\n",
    "# Find the total number of attributes\n",
    "num_attributes = len(attributes)\n",
    "\n",
    "# Create a sparse matrix\n",
    "sparse_matrix = lil_matrix((len(data_lines), num_attributes))\n",
    "\n",
    "# Regular expression to parse the sparse format\n",
    "sparse_pattern = re.compile(r'(\\d+)\\s+(\\d+)')\n",
    "\n",
    "# Fill the sparse matrix with data\n",
    "for i, row in enumerate(data_lines):\n",
    "    matches = sparse_pattern.findall(row)\n",
    "    for index, value in matches:\n",
    "        sparse_matrix[i, int(index)] = int(value)\n",
    "\n",
    "# Convert the sparse matrix to a dense DataFrame\n",
    "df1 = pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n",
    "\n",
    "# Set the column names\n",
    "df1.columns = attributes\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0150ddb",
   "metadata": {},
   "source": [
    "## Loading ENRON Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd36a4e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd36a4e9",
    "outputId": "431a1b84-671b-4728-b748-b7d7859519e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A.A8  C.C9  B.B12  C.C11  C.C5  C.C7  B.B2  B.B3  D.D16  A.A7  ...  \\\n",
      "0        0     0      0      0     0     0     0     0      0     0  ...   \n",
      "1        0     0      0      0     0     0     0     0      0     0  ...   \n",
      "2        0     0      0      0     0     0     0     0      0     0  ...   \n",
      "3        0     0      0      0     0     0     0     0      0     0  ...   \n",
      "4        0     0      0      0     0     0     0     0      0     0  ...   \n",
      "...    ...   ...    ...    ...   ...   ...   ...   ...    ...   ...  ...   \n",
      "1698     0     0      0      0     0     0   1.0     0      0     0  ...   \n",
      "1699     0     0      0      0     0     0   1.0     0      0     0  ...   \n",
      "1700     0     0      0      0     0     0   1.0     0      0     0  ...   \n",
      "1701     0   1.0      0      0     0     0   1.0     0      0     0  ...   \n",
      "1702     0     0      0      0     0     0   1.0     0      0     0  ...   \n",
      "\n",
      "      workers  working  world  writer  writers  www  year  years  yesterday  \\\n",
      "0           0        0      0       0        0    0     0      0          0   \n",
      "1           0        0      0       0        0    0     0      0          0   \n",
      "2           0        0      0       0        0    0     0      0          0   \n",
      "3           0        0      0       0        0    0     0      0          0   \n",
      "4           0        0      0       0        0    0     0      0          0   \n",
      "...       ...      ...    ...     ...      ...  ...   ...    ...        ...   \n",
      "1698        0        0      0       0        0    0   1.0    1.0          0   \n",
      "1699        0        0      0       0        0  1.0   1.0    1.0        1.0   \n",
      "1700        0        0      0       0        0    0   1.0      0          0   \n",
      "1701        0        0      0       0        0  1.0   1.0    1.0        1.0   \n",
      "1702        0        0      0       0        0  1.0   1.0    1.0        1.0   \n",
      "\n",
      "      york  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "...    ...  \n",
      "1698     0  \n",
      "1699     0  \n",
      "1700     0  \n",
      "1701     0  \n",
      "1702     0  \n",
      "\n",
      "[1703 rows x 1054 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "import re\n",
    "\n",
    "# Path to the ARFF file\n",
    "arff_file_path = r'ENRON-F.arff'\n",
    "\n",
    "# Read the ARFF file\n",
    "with open(arff_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Initialize variables to store the attributes and data\n",
    "attributes = []\n",
    "data_lines = []\n",
    "\n",
    "# Flag to indicate when data section starts\n",
    "data_section = False\n",
    "\n",
    "# Process each line to extract attributes and data\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith('@attribute'):\n",
    "        attr_name = line.split()[1]\n",
    "        attributes.append(attr_name)\n",
    "    elif line.startswith('@data'):\n",
    "        data_section = True\n",
    "    elif data_section:\n",
    "        data_lines.append(line)\n",
    "\n",
    "# Find the total number of attributes\n",
    "num_attributes = len(attributes)\n",
    "\n",
    "# Create a sparse matrix\n",
    "sparse_matrix = lil_matrix((len(data_lines), num_attributes))\n",
    "\n",
    "# Regular expression to parse the sparse format\n",
    "sparse_pattern = re.compile(r'(\\d+)\\s+(\\d+)')\n",
    "\n",
    "# Fill the sparse matrix with data\n",
    "for i, row in enumerate(data_lines):\n",
    "    matches = sparse_pattern.findall(row)\n",
    "    for index, value in matches:\n",
    "        sparse_matrix[i, int(index)] = int(value)\n",
    "\n",
    "# Convert the sparse matrix to a dense DataFrame\n",
    "df2 = pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n",
    "\n",
    "# Set the column names\n",
    "df2.columns = attributes\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001398b",
   "metadata": {},
   "source": [
    "## Loading Medical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2499a830",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2499a830",
    "outputId": "31aeec3c-a911-4a2d-dd33-0d23b086cf6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -  /  0  00  04  0;  0cm    1  1-1/2  1-1/2-year  ...  Class-35-493_90  \\\n",
      "0      0  0  0   0   0   0    0    0      0           0  ...                0   \n",
      "1    1.0  0  0   0   0   0    0  1.0      0           0  ...                0   \n",
      "2    1.0  0  0   0   0   0    0  1.0      0           0  ...                0   \n",
      "3    1.0  0  0   0   0   0    0    0      0           0  ...                0   \n",
      "4      0  0  0   0   0   0    0  1.0      0           0  ...                0   \n",
      "..   ... .. ..  ..  ..  ..  ...  ...    ...         ...  ...              ...   \n",
      "973    0  0  0   0   0   0    0    0      0           0  ...              1.0   \n",
      "974  1.0  0  0   0   0   0    0    0      0           0  ...                0   \n",
      "975  1.0  0  0   0   0   0    0    0      0           0  ...                0   \n",
      "976    0  0  0   0   0   0    0    0      0           0  ...                0   \n",
      "977  1.0  0  0   0   0   0    0    0      0           0  ...                0   \n",
      "\n",
      "     Class-36-788_30  Class-37-753_3  Class-38-593_89  Class-39-758_6  \\\n",
      "0                  0               0                0               0   \n",
      "1                  0               0                0               0   \n",
      "2                1.0               0                0               0   \n",
      "3                  0               0                0               0   \n",
      "4                  0               0                0               0   \n",
      "..               ...             ...              ...             ...   \n",
      "973                0               0                0               0   \n",
      "974                0               0                0               0   \n",
      "975                0               0                0               0   \n",
      "976                0               0                0               0   \n",
      "977                0               0              1.0               0   \n",
      "\n",
      "     Class-40-741_90  Class-41-591  Class-42-599_7  Class-43-279_12  \\\n",
      "0                  0             0               0                0   \n",
      "1                  0             0               0                0   \n",
      "2                  0           1.0               0                0   \n",
      "3                  0             0               0                0   \n",
      "4                  0             0               0                0   \n",
      "..               ...           ...             ...              ...   \n",
      "973                0             0               0                0   \n",
      "974                0             0               0                0   \n",
      "975                0             0               0                0   \n",
      "976                0             0               0                0   \n",
      "977                0             0               0                0   \n",
      "\n",
      "     Class-44-786_07  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "..               ...  \n",
      "973                0  \n",
      "974              1.0  \n",
      "975                0  \n",
      "976              1.0  \n",
      "977                0  \n",
      "\n",
      "[978 rows x 1494 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "import re\n",
    "\n",
    "# Path to the ARFF file\n",
    "arff_file_path = r'medical.arff'\n",
    "\n",
    "# Read the ARFF file\n",
    "with open(arff_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Initialize variables to store the attributes and data\n",
    "attributes = []\n",
    "data_lines = []\n",
    "\n",
    "# Flag to indicate when data section starts\n",
    "data_section = False\n",
    "\n",
    "# Process each line to extract attributes and data\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith('@attribute'):\n",
    "        attr_name = line.split()[1]\n",
    "        attributes.append(attr_name)\n",
    "    elif line.startswith('@data'):\n",
    "        data_section = True\n",
    "    elif data_section:\n",
    "        data_lines.append(line)\n",
    "\n",
    "# Find the total number of attributes\n",
    "num_attributes = len(attributes)\n",
    "\n",
    "# Create a sparse matrix\n",
    "sparse_matrix = lil_matrix((len(data_lines), num_attributes))\n",
    "\n",
    "# Regular expression to parse the sparse format\n",
    "sparse_pattern = re.compile(r'(\\d+)\\s+(\\d+)')\n",
    "\n",
    "# Fill the sparse matrix with data\n",
    "for i, row in enumerate(data_lines):\n",
    "    matches = sparse_pattern.findall(row)\n",
    "    for index, value in matches:\n",
    "        sparse_matrix[i, int(index)] = int(value)\n",
    "\n",
    "# Convert the sparse matrix to a dense DataFrame\n",
    "df3 = pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n",
    "\n",
    "# Set the column names\n",
    "df3.columns = attributes\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65245eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19301, 1026)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce68f4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1703, 1054)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f2b7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978, 1494)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18581c",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cb12f5",
   "metadata": {
    "id": "76cb12f5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import hamming_loss, average_precision_score\n",
    "\n",
    "# Hamming Loss\n",
    "def compute_hamming_loss(y_true, y_pred):\n",
    "    return hamming_loss(y_true, y_pred)\n",
    "\n",
    "# One Error\n",
    "def compute_one_error(y_true, y_prob):\n",
    "    one_error = 0\n",
    "    for i in range(len(y_true)):\n",
    "        top_label = np.argmax(y_prob[i])\n",
    "        if y_true[i][top_label] == 0:\n",
    "            one_error += 1\n",
    "    return one_error / len(y_true)\n",
    "\n",
    "# Coverage\n",
    "def compute_coverage(y_true, y_prob):\n",
    "    coverage = 0\n",
    "    for i in range(len(y_true)):\n",
    "        sorted_indices = np.argsort(y_prob[i])[::-1]\n",
    "        true_indices = np.where(y_true[i] == 1)[0]\n",
    "        if len(true_indices) == 0:\n",
    "            continue  # Skip if there are no true labels for this sample\n",
    "        max_index = max([np.where(sorted_indices == idx)[0][0] for idx in true_indices])\n",
    "        coverage += max_index\n",
    "    return (coverage + 1) / len(y_true)\n",
    "\n",
    "# Ranking Loss\n",
    "def compute_ranking_loss(y_true, y_prob):\n",
    "    ranking_loss = 0\n",
    "    total_pairs = 0\n",
    "    for i in range(len(y_true)):\n",
    "        relevant = np.where(y_true[i] == 1)[0]\n",
    "        irrelevant = np.where(y_true[i] == 0)[0]\n",
    "        if len(relevant) == 0 or len(irrelevant) == 0:\n",
    "            continue  # Skip if either relevant or irrelevant labels are empty\n",
    "        total_pairs += len(relevant) * len(irrelevant)\n",
    "        for rel in relevant:\n",
    "            for irrel in irrelevant:\n",
    "                if y_prob[i][rel] <= y_prob[i][irrel]:\n",
    "                    ranking_loss += 1\n",
    "    if total_pairs == 0:\n",
    "        return 0  # Return 0 if there are no pairs to compute loss\n",
    "    return ranking_loss / total_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Average Precision\n",
    "def compute_average_precision(y_true, y_prob):\n",
    "    return np.mean([average_precision_score(y_true[i], y_prob[i]) for i in range(len(y_true))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a4964",
   "metadata": {},
   "source": [
    "## ML-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e14d4e9",
   "metadata": {
    "id": "4e14d4e9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, hamming_loss\n",
    "\n",
    "def ml_knn(S, k, s):\n",
    "    # Split the dataset into features and labels\n",
    "    labels = S.iloc[:, :20].values\n",
    "    features = S.iloc[:, 20:].values\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize variables\n",
    "    m = y_train.shape[1]  # Number of labels\n",
    "    N = len(y_train)  # Number of training samples\n",
    "    Y_pred = []\n",
    "    Y_prob = []\n",
    "\n",
    "    # Computing the prior probabilities P(Hj)\n",
    "    P_Hj = np.zeros(m)\n",
    "    for j in range(m):\n",
    "        Cj = np.sum(y_train[:, j])\n",
    "        P_Hj[j] = (Cj + s) / (N + s * 2)\n",
    "\n",
    "    P_Hj_bar = 1 - P_Hj\n",
    "\n",
    "    # Computing the posterior probabilities P(Ej|Hj), P(Ej|H̅j)\n",
    "    P_E_Hj = np.zeros((m, k+1))\n",
    "    P_E_Hj_bar = np.zeros((m, k+1))\n",
    "    neighbors = NearestNeighbors(n_neighbors=k).fit(X_train)\n",
    "\n",
    "    for j in range(m):\n",
    "        c = np.zeros(k + 1)\n",
    "        c_bar = np.zeros(k + 1)\n",
    "        for index in range(N):\n",
    "            distances, indices = neighbors.kneighbors([X_train[index]], return_distance=True)\n",
    "            delta = np.sum(y_train[indices[0], j])\n",
    "            if y_train[index, j] == 1:\n",
    "                c[int(delta)] += 1\n",
    "            else:\n",
    "                c_bar[int(delta)] += 1\n",
    "\n",
    "        for i in range(k + 1):\n",
    "            P_E_Hj[j][i] = (c[i] + s) / (np.sum(c) + s * (k + 1))\n",
    "            P_E_Hj_bar[j][i] = (c_bar[i] + s) / (np.sum(c_bar) + s * (k + 1))\n",
    "\n",
    "    # Making predictions for the test set\n",
    "    for index in range(len(X_test)):\n",
    "        N_k = neighbors.kneighbors([X_test[index]], return_distance=False)\n",
    "        E_prime = np.sum(y_train[N_k[0]], axis=0)\n",
    "\n",
    "        Y_temp = []\n",
    "        Y_temp_prob = []\n",
    "        for j in range(m):\n",
    "            if E_prime[j] > k:\n",
    "                E_prime[j] = k  # Cap the value at k\n",
    "\n",
    "            prob_Hj = P_Hj[j] * P_E_Hj[j][int(E_prime[j])]\n",
    "            prob_Hj_bar = P_Hj_bar[j] * P_E_Hj_bar[j][int(E_prime[j])]\n",
    "\n",
    "            # Normalize to get probabilities\n",
    "            total_prob = prob_Hj + prob_Hj_bar\n",
    "            prob_label_1 = prob_Hj / total_prob\n",
    "\n",
    "            if prob_label_1 > 0.5:\n",
    "                Y_temp.append(1)\n",
    "            else:\n",
    "                Y_temp.append(0)\n",
    "\n",
    "            Y_temp_prob.append(prob_label_1)\n",
    "\n",
    "        Y_pred.append(Y_temp)\n",
    "        Y_prob.append(Y_temp_prob)\n",
    "\n",
    "\n",
    "    return y_test, Y_pred, Y_prob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61b93d6",
   "metadata": {
    "id": "b61b93d6"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_results_to_csv(y_test, y_pred, y_prob, file_name):\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['y_test', 'y_pred', 'y_prob'])\n",
    "        for y_t, y_p, y_pr in zip(y_test, y_pred, y_prob):\n",
    "            writer.writerow([y_t, y_p, y_pr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b44b1a5",
   "metadata": {
    "id": "6b44b1a5"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame with the first 20 columns as labels and the rest as features\n",
    "y_test1, Y_pred1 , y_prob1 = ml_knn(df1, k=7, s=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab367496",
   "metadata": {
    "id": "ab367496"
   },
   "outputs": [],
   "source": [
    "save_results_to_csv(y_test1, Y_pred1, y_prob1, 'results1.csv')\n",
    "# files.download('results1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9bb0d6f",
   "metadata": {
    "id": "b9bb0d6f"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame with the first 20 columns as labels and the rest as features\n",
    "y_test2, Y_pred2 , y_prob2 = ml_knn(df2, k=7, s=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bf39322",
   "metadata": {
    "id": "8bf39322"
   },
   "outputs": [],
   "source": [
    "save_results_to_csv(y_test2, Y_pred2, y_prob2, 'results2.csv')\n",
    "# files.download('results2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "261f0875",
   "metadata": {
    "id": "261f0875"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame with the first 20 columns as labels and the rest as features\n",
    "y_test3, Y_pred3 , y_prob3 = ml_knn(df3, k=7, s=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81343251",
   "metadata": {
    "id": "81343251"
   },
   "outputs": [],
   "source": [
    "save_results_to_csv(y_test3, Y_pred3, y_prob3, 'results3.csv')\n",
    "# files.download('results3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8574fd",
   "metadata": {},
   "source": [
    "## Evaluating 20NG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6c51450",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6c51450",
    "outputId": "0a7f2a37-5955-4f63-8d91-dcc352d0a173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.0407\n",
      "One Error: 0.5242\n",
      "Coverage: 3.6037\n",
      "Ranking Loss: 0.1862\n",
      "Average Precision: 0.5928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Hamming Loss\n",
    "hamming = compute_hamming_loss(y_test1, Y_pred1)\n",
    "\n",
    "# One Error\n",
    "one_error = compute_one_error(y_test1, y_prob1)\n",
    "\n",
    "# Coverage\n",
    "coverage = compute_coverage(y_test1, y_prob1)\n",
    "\n",
    "# Ranking Loss\n",
    "ranking_loss = compute_ranking_loss(y_test1, y_prob1)\n",
    "\n",
    "# Average Precision\n",
    "average_precision = label_ranking_average_precision_score(y_test1, y_prob1)\n",
    "\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"One Error: {one_error:.4f}\")\n",
    "print(f\"Coverage: {coverage:.4f}\")\n",
    "print(f\"Ranking Loss: {ranking_loss:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad37dba",
   "metadata": {},
   "source": [
    "## Evaluating ENRON Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a523c60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a523c60",
    "outputId": "9a5be7ef-959a-4f9d-a7d7-4a2c7592acb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.0630\n",
      "One Error: 0.3314\n",
      "Coverage: 2.9795\n",
      "Ranking Loss: 0.0846\n",
      "Average Precision: 0.7618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Hamming Loss\n",
    "hamming = compute_hamming_loss(y_test2, Y_pred2)\n",
    "\n",
    "# One Error\n",
    "one_error = compute_one_error(y_test2, y_prob2)\n",
    "\n",
    "# Coverage\n",
    "coverage = compute_coverage(y_test2, y_prob2)\n",
    "\n",
    "# Ranking Loss\n",
    "ranking_loss = compute_ranking_loss(y_test2, y_prob2)\n",
    "\n",
    "# Average Precision\n",
    "average_precision = label_ranking_average_precision_score(y_test2, y_prob2)\n",
    "\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"One Error: {one_error:.4f}\")\n",
    "print(f\"Coverage: {coverage:.4f}\")\n",
    "print(f\"Ranking Loss: {ranking_loss:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3789d",
   "metadata": {},
   "source": [
    "## Evaluating Medical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc1fad75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc1fad75",
    "outputId": "8bd28ff7-62cb-45df-9999-03bbc7d27f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.0296\n",
      "One Error: 0.6071\n",
      "Coverage: 0.7041\n",
      "Ranking Loss: 0.0468\n",
      "Average Precision: 0.9376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Hamming Loss\n",
    "hamming = compute_hamming_loss(y_test3, Y_pred3)\n",
    "\n",
    "# One Error\n",
    "one_error = compute_one_error(y_test3, y_prob3)\n",
    "\n",
    "# Coverage\n",
    "coverage = compute_coverage(y_test3, y_prob3)\n",
    "\n",
    "# Ranking Loss\n",
    "ranking_loss = compute_ranking_loss(y_test3, y_prob3)\n",
    "\n",
    "# Average Precision\n",
    "average_precision = label_ranking_average_precision_score(y_test3, y_prob3)\n",
    "\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"One Error: {one_error:.4f}\")\n",
    "print(f\"Coverage: {coverage:.4f}\")\n",
    "print(f\"Ranking Loss: {ranking_loss:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
